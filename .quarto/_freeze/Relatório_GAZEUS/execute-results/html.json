{
  "hash": "502f391744400b8e757c7114394b7aab",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Teste Técnico\" # Título do relatório\nsubtitle: \"**Posição de Data Analyst Jr - Buraco Jogatina**\"\nauthor: \"Fernanda Kelly R. Silva\" # Autor(a)\nlang: pt \ndate: \"2024-03-24\" \ndate-format: short \ntoc: true \nformat: \n    html: \n      #css: [\"custom.css\"] \n      code-fold: false \n      code-tools: true  \n      theme: \n        light: cosmo\n        dark: superhero \n#title-block-banner: \"#874a9c\" \ncode-annotations: hover \nexecute:\n  warning: false\n  message: false\n---\n\n::: {.cell}\n\n:::\n\n\n\n# Tarefa 01: SQL\n\nUm sistema de pagamentos, registra as transações dos seus usuários em duas tabelas:\n\nUsers: Contém o registro de usuários do sistema. Os seus atributos são:\n\n-   id: Identificador único de usuário\n-   age (idade): em anos\n-   país de residência: país onde o usuários indicou que reside\n\nTransactions: Que contém o registro de todas as transações que os usuários realizam através do sistema.\n\n-   transaction_id: Identificador único da transação\n-   transaction_date: Data na qual a transação foi realizada. O tipo é timestamp, ou seja, é um tipo de data que contém o ano, mês, dia, hora, minuto e segundos.\n-   user_id: Identificador do usuário que realizou a transação (é uma chave externa à tabela users)\n-   transaction_state: Campo que indica o estado da transação que pode ser *INITIATED*, *SUCCESS*, *FRAUD* or *CANCELLED*.\n-   transaction_amount: Valor da transação em USD\n\nCom estes dados disponíveis, precisamos gerar queries SQL para responder às seguintes perguntas. **Para fins didáticos, criei esses bancos de dados para simular a resolução das questões.**\n\n### tabela_transactions\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::kable(tabela_transactions)\n```\n\n::: {.cell-output-display}\n\n\n|user_id |transaction_date        |transaction_id |transaction_state | transaction_amount|data_curta |\n|:-------|:-----------------------|:--------------|:-----------------|------------------:|:----------|\n|1       |2024-03-12 15:08:49 UTC |255            |sucesso           |             119.65|2024-03-12 |\n|2       |2024-03-23 20:08:03 UTC |585            |INITIATED         |              22.63|2024-03-23 |\n|3       |2024-03-18 10:56:25 UTC |178            |CANCELLED         |             225.87|2024-03-18 |\n|4       |2024-03-12 08:21:32 UTC |887            |SUCCESS           |              16.58|2024-03-12 |\n|5       |2024-03-30 05:24:49 UTC |569            |FRAUD             |            1225.89|2024-03-30 |\n|6       |2024-03-21 12:32:51 UTC |789            |SUCCESS           |              59.90|2024-03-21 |\n|7       |2024-03-10 08:25:49 UTC |236            |FRAUD             |             256.98|2024-03-10 |\n|7       |2024-03-10 05:21:49 UTC |458            |FRAUD             |             986.69|2024-03-10 |\n|7       |2024-03-09 01:08:49 UTC |125            |FRAUD             |            1256.89|2024-03-09 |\n\n\n:::\n:::\n\n\n\n### tabela_user\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::kable(tabela_user)\n```\n\n::: {.cell-output-display}\n\n\n|id |age |pais           |\n|:--|:---|:--------------|\n|1  |18  |Brasil         |\n|2  |29  |Estados Unidos |\n|3  |55  |África do Sul  |\n|4  |32  |Bolívia        |\n|5  |60  |Brasil         |\n|6  |42  |Brasil         |\n|7  |38  |África do Sul  |\n\n\n:::\n:::\n\n\n\nPara que a experiência do SQL seja real, vou conectar as bases com o SQLite.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dbplyr)\nlibrary(dplyr)\n\ncon <- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\ncopy_to(con, tabela_user)\ncopy_to(con, tabela_transactions)\n\ntabela_user <- tbl(con, \"tabela_user\")                 # <1>\ntabela_transactions <- tbl(con, \"tabela_transactions\") # <2>\n```\n:::\n\n\n\n1.  Conectando a base de dados {tabela_user} com o SQLite\n2.  Conectando a base de dados {tabela_transactions} com o SQLite\n\n::: panel-tabset\n# A\n\nQual é a idade média de usuários do sistema por país?\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: callout-tip\n# Resposta\n:::\n\n``` sql\n<SQL>\nSELECT `pais`, ROUND(AVG(`age`), 2) AS `media_idade`\nFROM `tabela_user`\nGROUP BY `pais`\n```\n\n# B\n\nQual é o país com a maior quantidade de dinheiro transacionado (considere só transações finalizadas com sucesso ou \"SUCCESS\")?\n\nComo vamos precisar do merge das duas tabelas, vou criar a **tabela_merge** sem algum filtro para facilitar as consultas e não alocar memória desnecessária.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntabela_merge <- tabela_transactions %>%\n                 dplyr::left_join(tabela_user, by = c(\"user_id\" = \"id\")) \n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: callout-tip\n# Resposta\n:::\n\n``` sql\n<SQL>\nSELECT\n  `user_id`,\n  `transaction_date`,\n  `transaction_id`,\n  `transaction_state`,\n  `transaction_amount`,\n  `data_curta`,\n  `age`,\n  `pais`\nFROM (\n  SELECT `q01`.*, RANK() OVER (ORDER BY `transaction_amount` DESC) AS `col01`\n  FROM (\n    SELECT `tabela_transactions`.*, `age`, `pais`\n    FROM `tabela_transactions`\n    LEFT JOIN `tabela_user`\n      ON (`tabela_transactions`.`user_id` = `tabela_user`.`id`)\n  ) AS `q01`\n  WHERE (`transaction_state` = 'SUCCESS' OR `transaction_state` = 'sucesso')\n) AS `q01`\nWHERE (`col01` <= 1)\n```\n\n# C\n\nQual é o país com maior taxa de fraude em porcentagem respeito ao número de transações totais no país?\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: callout-tip\n# Resposta\n:::\n\n``` sql\n<SQL>\nSELECT\n  `user_id`,\n  `transaction_date`,\n  `transaction_id`,\n  `transaction_state`,\n  `transaction_amount`,\n  `data_curta`,\n  `age`,\n  `pais`,\n  `taxa`\nFROM (\n  SELECT `q01`.*, RANK() OVER (ORDER BY `taxa` DESC) AS `col01`\n  FROM (\n    SELECT\n      `q01`.*,\n      `transaction_amount` / SUM(`transaction_amount`) OVER (PARTITION BY `transaction_state`) AS `taxa`\n    FROM (\n      SELECT `tabela_transactions`.*, `age`, `pais`\n      FROM `tabela_transactions`\n      LEFT JOIN `tabela_user`\n        ON (`tabela_transactions`.`user_id` = `tabela_user`.`id`)\n    ) AS `q01`\n  ) AS `q01`\n  WHERE (`transaction_state` = 'FRAUD')\n) AS `q01`\nWHERE (`col01` <= 1)\n```\n\n# D\n\nNa mesma linha da pergunta anterior, responda qual é a faixa de idade de usuários que mais cometem fraude (em percentagem). Separe as faixas etárias em: (< 18 anos, 18-30 anos, 30 - 45 anos, 45 - 60 anos, 60 > anos) e considere que para responder essa pergunta, você deverá considerar o fato que um usuário pode ter executado várias transações, das quais poucas (ou muitas) podem ter sido fraude entre as demais.\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: callout-tip\n# Resposta\n:::\n\n``` sql\nSELECT\n  `user_id`,\n  `transaction_date`,\n  `transaction_id`,\n  `transaction_state`,\n  `transaction_amount`,\n  `data_curta`,\n  `age`,\n  `pais`,\n  `age_cat`,\n  `taxa_idade`\nFROM (\n  SELECT `q01`.*, RANK() OVER (ORDER BY `taxa_idade` DESC) AS `col01`\n  FROM (\n    SELECT\n      `q01`.*,\n      CASE\nWHEN (`age` < 18.0) THEN 'menor que 18 anos'\nWHEN (`age` >= 18.0 AND `age` < 30.0) THEN '18-29 anos'\nWHEN (`age` >= 30.0 AND `age` < 45.0) THEN '30-44 anos'\nWHEN (`age` >= 45.0 AND `age` < 60.0) THEN '45-59 anos'\nWHEN (`age` >= 60.0) THEN 'maior igual a 60 anos'\nEND AS `age_cat`,\n      CASE WHEN (`transaction_state` = 'FRAUD') THEN (`transaction_amount` / SUM(`transaction_amount`) OVER ()) WHEN NOT (`transaction_state` = 'FRAUD') THEN 0.0 END AS `taxa_idade`\n    FROM (\n      SELECT `tabela_transactions`.*, `age`, `pais`\n      FROM `tabela_transactions`\n      LEFT JOIN `tabela_user`\n        ON (`tabela_transactions`.`user_id` = `tabela_user`.`id`)\n    ) AS `q01`\n  ) AS `q01`\n) AS `q01`\nWHERE (`col01` <= 1)\n```\n\n# E\n\nImagine que a camada executiva da empresa dona do sistema, precisa criar um Dashboard para monitorar o estado das transações nos últimos 3 dias. Para isso você precisa criar uma query SQL que calcule o número e dinheiro das transações não finalizadas, número e dinheiro de transações finalizadas com sucesso (SUCCESS), o número e dinheiro de transações canceladas (CANCELLED), o número e dinheiro de fraudes (FRAUD), agrupado por país, nos 3 dias anteriores de quando o executivo da empresa consulte seu Dashboard.\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: callout-tip\n# Resposta\n:::\n\n``` sql\n<SQL>\nSELECT\n  `q01`.*,\n  COUNT(*) OVER `win1` AS `n_transacoes`,\n  SUM(`transaction_amount`) OVER `win1` AS `dinheiro`\nFROM (\n  SELECT\n    `user_id`,\n    `transaction_date`,\n    `transaction_id`,\n    CASE WHEN (`transaction_state` = 'sucesso') THEN 'SUCCESS' WHEN NOT (`transaction_state` = 'sucesso') THEN `transaction_state` END AS `transaction_state`,\n    `transaction_amount`,\n    `data_curta`,\n    `age`,\n    `pais`,\n    `tres_n`\n  FROM (\n    SELECT\n      `q01`.*,\n      CASE WHEN ((DATE('now') - 1.0) = `data_curta`) THEN 1.0 WHEN NOT ((DATE('now') - 1.0) = `data_curta`) THEN (CASE WHEN ((DATE('now') - 2.0) = `data_curta`) THEN 1.0 WHEN NOT ((DATE('now') - 2.0) = `data_curta`) THEN (CASE WHEN ((DATE('now') - 2.0) = `data_curta`) THEN 1.0 WHEN NOT ((DATE('now') - 2.0) = `data_curta`) THEN 0.0 END) END) END AS `tres_n`\n    FROM (\n      SELECT `tabela_transactions`.*, `age`, `pais`\n      FROM `tabela_transactions`\n      LEFT JOIN `tabela_user`\n        ON (`tabela_transactions`.`user_id` = `tabela_user`.`id`)\n    ) AS `q01`\n  ) AS `q01`\n  WHERE (`tres_n` = 1.0)\n) AS `q01`\nWINDOW `win1` AS (PARTITION BY `pais`, `transaction_state`)\n```\n:::\n\n# Tarefa 02: Python\n\n\n\n::: {.cell}\n\n:::\n\n\n\nNuma tribo ancestral e muito desenvolvida, orientada principalmente por uma cultura lógica e matemática, os nativos estão interessados no estudo das linguagens de outras tribos e civilizações.\n\nDurante seus estudos, estes perceberam que as linguagens dessas outras civilizações são compostas por um conjunto de símbolos (letras do alfabeto) que são agrupados e combinados para representar conceitos (palavras). Assim mesmo, estes também sabem que cada civilização possui um alfabeto específico.\n\nNa tentativa da tribo de avançar nos seus estudos, estes desejam saber qual seria o número de palavras possíveis a serem criadas em função do tamanho da palavra e o conjunto de símbolos (alfabeto) da civilização em estudo, independentemente se essas \"palavras\" representam algum significado ou não.\n\n::: panel-tabset\n# A\n\nVocê, que compartilha os atributos da tribo enquanto as capacidades analíticas e lógicas, precisa ajudá-la escrevendo um algoritmo para fazer o cálculo descrito acima, considerando que tribo oferecerá para você o alfabeto da civilização e o tamanho da palavra/combinação. Seu algoritmo deveria ser capaz de calcular que o número total de palavras possíveis (independente se tem significado ou não) é de 16.\n\nOBS: NÃO PRECISA ESCREVER A LISTA DAS PALAVRAS, A TRIBO SÓ PRECISA DO NÚMERO!\n\n::: callout-tip\n# Resposta\n\nEsse é um claro problema de análise combinatória com reposição/reposição.\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom itertools import combinations_with_replacement,chain\n\n# Obtem todas as combinações de [1, 2, 3] e tamanho 2\ncomb_A = combinations_with_replacement([1, 2, 3], 2)\nprint(len(list(comb_A)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n6\n```\n\n\n:::\n:::\n\n\n\n# B\n\nPara ajudar a tribo ainda mais! Você deverá modificar seu algoritmo (ou talvez escrever um novo) para fazer o mesmo cálculo, só que agora, as palavras não podem ter símbolos repetidos.\n\n::: callout-tip\n# Resposta\n\nEsse também é um claro problema de análise combinatória, mas, nesse caso, é um problema em que **não há reposição**.\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom itertools import combinations\n\n# Obtem todas as permutações de {@,d,2,b}  \ncomb_B = combinations([1, 2, 3], 2)\n\nprint(len(list(comb_B)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3\n```\n\n\n:::\n:::\n\n\n:::\n\n# Tarefa 03: Estatística e Machine Learning\n\n::: panel-tabset\n# 1\n\nDado um dataset das estaturas (em centímetros) de 13 indivíduos:\n\nDataset = {175, 166, 183, 193, 155, 177, 173, 171, 162, 185, 176, 161, 188}\n\nCalcule:\n\n-   a média\n-   a mediana\n-   20 percentil\n-   80 percentil\n-   desvio padrão\n\nConsiderando os dados, responda a que distribuição (paramétrica) os dados se aproximam.\n\n::: callout-tip\n# Resposta\n:::\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n| vinte_percentil|     mean| median| oitenta_percentil| desvio_padrao|\n|---------------:|--------:|------:|-----------------:|-------------:|\n|           163.6| 174.2308|    175|             184.2|      11.25577|\n\n\n:::\n:::\n\n\n\nDe acordo com o gráfico abaixo, a distribuição da variável **altura** pode ser descrita por uma normal, mas para fins de decisão, deve-se munir de testes paramétricos ou não paramétricos. Veja que sua variabilidade indica que os valores observados tendem a estar distantes da média – ou seja, a distribuição é mais “espalhada”.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Relatório_GAZEUS_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nqqnorm(dataset$altura)\nqqline(dataset$altura, col=2)\n```\n\n::: {.cell-output-display}\n![](Relatório_GAZEUS_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\n# 2\n\nCom suas próprias palavras, explique em que consiste o teorema central do limite, e se possível, mencione a sua importância no campo da inferência estatística.\n\n::: callout-tip\n# Resposta\n:::\n\nO Teorema do limite central (TLC) basicamente demonstra a **tendência** de aproximação das variáveis aleatórias com a distribuição normal, em que a aproximação da distribuição Normal melhora na medida que se fizesse a média do lançamento de mais dados. Esse comportamento pode ser visualizado na imagem abaixo. Na estatística, há provas que um n igual a 30 é suficientemente grande para que essa amostra seja descrita com uma distribuição normal e, por isso, é fácil encontrar análises clássicas com todos os pressupostos violados por seus pesquisadores devido a esta pressuposição do n igual a 30.\n\n![Teorema do Limite Central](img/img213.png){fig-align=\"center\" width=\"300\"}\n\n# 3\n\nSuponha que numa escola, 2 grupos diferentes de estudantes (Grupo A e Grupo B) fazem o mesmo teste de matemáticas. As pontuações para cada grupo são dados pelos seguintes datasets:\n\n-   Grupo A: {80, 85, 88, 90, 92, 75, 78}\n-   Grupo B: {75, 78, 82, 85, 87, 93, 99}\n\nElabore um teste de hipótese para determinar se existe uma diferença estatisticamente significativa entre a média das pontuações dos dois grupos com uma confiança de 95%.\n\n::: callout-tip\n# Resposta\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n**Antes de qualquer levantamento de hipótese, é bom analisar as medidas de tendência central de cada grupo e também graficamente** e, de acordo com a tabela abaixo, não há muitas evidências de que essa média entre os grupos seja diferente, mas é evidente que o **grupo B** tem uma variabilidade maior em relação as notas (desvio padrão = 8.363754).\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n| vinte_percentil|     mean| median| oitenta_percentil| desvio_padrao|\n|---------------:|--------:|------:|-----------------:|-------------:|\n|           163.6| 174.2308|    175|             184.2|      11.25577|\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](Relatório_GAZEUS_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\nÉ importante ressaltar que a verificação dos pressupostos é de extrema relevância para que os insights e conclusões sejam verdadeiramente corretos. Neste casso, estamos **supondo ** que as duas amostras são independentes e, em caso de teste de médias, usaríamos o teste t independente que apresenta dois **pressupostos**:\n\n- **Normalidade da variável dependente de cada grupo**\n- **Homogeneidade de variâncias (ou seja, grupos com variâncias homogêneas)**\n\n### Normalidade\n\nPara verificar a normalidade dos dados, é recomendável utilizar o teste de **Shapiro WIlk** quando o número de amostra é menor que 50. A estatı́stica T (ou T ) é basicamente o quadrado de um coeficiente de correlação, onde o coeficiente de correlação de Pearson é calculado entre a estatı́stica ordenada X i na amostra e o escore ai , que representa o que a estatı́stica ordenada deveria parecer se a população é Normal. Portanto, as hipóteses levantadas são:\n\n$H_{0}$ : F(x) é função de distribuição normal;\n$H_{1}$ : F(x) não é função de distribuição normal;\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|grupo |variable | statistic|         p|\n|:-----|:--------|---------:|---------:|\n|A     |nota     | 0.9434822| 0.6702574|\n|B     |nota     | 0.9727151| 0.9172920|\n\n\n:::\n:::\n\n\n\n\nVeja que de acordo com o teste de Shapiro Wilk, ao nível de 95% de significância, **não rejeita-se a hipótese nula**, ou seja, a variável **notas** é descrita por uma distribuição normal.\n\n\n### Variância\n\nO segundo pressuposto pode ser avaliado pelo **teste de Levene**, em que as hipóteses são:\n\n$H_{0}$ : Os grupos apresentam variâncias homogêneas;\n$H_{1}$ : Os grupos não apresentam variâncias homogêneas;\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|      | Df|   F value|   Pr(>F)|\n|:-----|--:|---------:|--------:|\n|group |  1| 0.2060638| 0.657971|\n|      | 12|        NA|       NA|\n\n\n:::\n:::\n\n\n\nLogo, ao nível de 95% de significância, **não rejeitamos a hipótese nula**, ou seja, os grupos apresentam variâncias homogêneas e assim podemos seguir com o teste t independente.\n\nSeguindo os passos para elaboração de um teste de hipótese:\n\n1.  Enunciar as Hipóteses:\n\n- $H_{0}$ : Não há diferença média entre os grupos; \n- $H_{1}$ : Há diferença média entre os grupos;\n\n2. Fixar o nı́vel de significância $\\alpha$) do teste:\n\n**No nosso caso, o nível de significância será 95%.**\n\n- $\\alpha$ : Erro tipo I: Rejeitar $H_{0}$  quando é verdadeira.\n\n\n3. Escolha da Estatı́stica teste adequada:\n\nPara responder a essa pergunta, usaremos o teste-t independente. Note que queremos comparar as notas (variáveis numéricas) de dois grupos independentes de alunos (A x B).\n\n4. Obtenção dos valores crı́ticos e Obtenção da Estatı́stica calculada:\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  nota by grupo\nt = -0.39353, df = 12, p-value = 0.7008\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -10.271845   7.128988\nsample estimates:\nmean in group A mean in group B \n       84.00000        85.57143 \n```\n\n\n:::\n:::\n\n\n\n6. Conclusão e tomada de decisão\n\nO teste resulta em um valor de t (no caso, t = -0.3935) e um valor de p (p-value) que é calculado com base nesse valor de t e nos graus de liberdade (df, do inglês degrees of freedom). Nesse caso, o valor de p é: 0.7008, um valor superior ao nível de significância 0.05. Como p > 0.05, não vamos rejeitar a $H_{0}$ e considerar que os dois grupos que apresentam notas, em média, não são estatisticamente diferentes. Veja que o IC 95% para a diferença entre médias inclui o zero: [-10.27; 7.12].\n\n\n# 4\n\nDetermine a hipótese nula (Ho) e hipótese alternativa (Ha) do seguinte cenário: Uma empresa afirma que o tempo médio dos seus produtos é menos de 4 dias. Você, conta com uma amostra dessas entregas para validar estadísticamente essa afirmação.\n\n::: callout-tip\n# Resposta\n:::\n\nNão é possível avaliar os pressupostos dessa amostra, mas considerando que esse tempo médio seja em dias (inteiros), o teste paramétrico sobre uma média populacional, a hipótese nula utilizada seria:\n\n\n- $H_{0}$ : $\\mu$ = 4;\n- $H_{1}$ : $\\mu$ $\\neq$ 4;\n\n\n# 5\n\nCalcule o coeficiente de correlação de pearson das variáveis \"Número de aparições Zendaya em filmes por ano\" e \"Número de pessoas afogadas em piscinas no Brasil ao ano\". Em base nos seus resultados, considera que a variável \"Número de aparições Zendaya em filmes por ano\" é um bom preditor do número de pessoas afogadas no Brasil? Justifique a sua resposta.\n\n::: callout-tip\n# Resposta\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\nA correlação entre duas variáveis pode ser verificada utilizando o teste de Pearson, caso há a suposição de normalidade nos dados. No caso em que a suposição de normalidade não é considerada ou a caracterı́stica dos próprio dados pressupõe isto (escala no mı́nimo ordinal), então pode-se fazer uso de estatı́sticas alternativas como: Teste de correlação por postos de Spearman ou de Kendall.\n\nVeja que de acordo com o gráfico abaixo que o \"Número de aparições Zendaya em filmes por ano\" e o \"Número de pessoas afogadas em piscinas no Brasil ao ano\" estão altamente correlacionados, mas faz senido essa correlçao?\n\nNa estatística, o nome que damos a essa correlação é **correlação espúria**. Uma correlação espúria é uma relação aparente entre duas variáveis que não possuem uma conexão causal real entre si. Essa relação ilusória pode surgir devido a fatores externos ou coincidências estatísticas, levando a conclusões equivocadas se não forem adequadamente analisadas.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(GGally)\nggpairs(tabela_5[,2:3], lower = list(continuous = \"smooth\"))\n```\n\n::: {.cell-output-display}\n![](Relatório_GAZEUS_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n\nA hipótese levantada é:\n\n- $H_{0}$ : $\\rho$ = 0;\n- $H_{1}$ : $\\rho$ $\\neq$ 0;\n\nVamos considerar o nível de significância a 95%. O teste de correção de Pearson é dados por,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstats::cor.test(tabela_5$zendaya, tabela_5$piscina,\n                method = \"pearson\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  tabela_5$zendaya and tabela_5$piscina\nt = 9.8414, df = 8, p-value = 9.563e-06\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8394382 0.9910229\nsample estimates:\n      cor \n0.9610948 \n```\n\n\n:::\n:::\n\n\n\nOu seja, ao nível de 95% de significância, há correlação entre as variáveis analisadas, como explicado acima, essa correlação é denominada por correlação espúria.\n\n# 6\n\nExplique a diferença entre amostragem estratificada e amostragem randômica ou aleatória. Discuta quais são as vantagens e desvantagens de cada uma e dê exemplos de casos onde uma abordagem é mais adequada que a outra.\n\n::: callout-tip\n# Resposta\n:::\n\nA **amostragem aleatória simples** é um método de seleção de n unidades de N, de modo que cada uma das $C_{n}$ amostras distintas tenha uma chance igual de ser sorteada. Na prática, uma amostra aleatória simples é extraída unidade por unidade. As unidades da população são numeradas de 1 a N. Uma série de números aleatórios entre 1 e N é então sorteada, seja por meio de uma tabela de números aleatórios ou por meio de um programa de computador que produz tal tabela. Em qualquer sorteio o processo utilizado deve dar uma chance igual de seleção a qualquer número da população ainda não sorteado. As unidades que contêm esses n números constituem a amostra. É facilmente verificado que todas as amostras distintas de $C_{n}$ têm chances iguais de serem selecionadas por este método. Considere uma amostra distinta, ou seja, um conjunto de n unidades especificadas. No primeiro sorteio, a probabilidade de que alguma das n unidades especificadas seja selecionada é n/ N. No segundo sorteio, a probabilidade de que alguma das unidades especificadas restantes (n -1) seja sorteada é (n -1)/( N-1), e assim por diante.\n\nComo um número sorteado é removido da população em todos os sorteios subsequentes, esse método também é chamado de amostragem aleatória sem reposição. A amostragem aleatória com reposição é inteiramente viável: em qualquer sorteio, todos os N membros da população têm a mesma chance de serem sorteados, independentemente da frequência com que já tenham sido sorteados. As fórmulas para as variâncias e variâncias estimadas das estimativas feitas a partir da amostra são muitas vezes mais simples quando a amostragem é com reposição do que quando é sem reposição. Por esta razão, a amostragem com reposição é por vezes utilizada nos planos amostrais mais complexos, embora à primeira vista pareça pouco sentido ter a mesma unidade duas ou mais vezes na amostra.\n\nNa **amostragem estratificada**, a população de N unidades é primeiro dividida em subpopulações de unidades $N_{1}, N_{2}, \\cdots, N_{L}$, respectivamente. Essas subpopulações não se sobrepõem e juntas compreendem toda a população, de modo que $N_{1} + N_{2} + \\cdots + N_{L} = N$. As subpopulações são chamadas de estratos. Para obter todos os benefícios da estratificação, os valores do $N_{h}$ devem ser conhecidos. Determinados os estratos, de cada um é retirada uma amostra, sendo os sorteios feitos de forma independente nos diferentes estratos. Os tamanhos das amostras dentro dos estratos são indicados por $n_{1}, n_{2}, \\cdots, n_{L}$ respectivamente. Se uma amostra aleatória simples for retirada em cada estrato, todo o procedimento é descrito como amostragem aleatória estratificada. A estratificação é uma técnica comum. Há muitas razões para isto, os principais são os seguintes.\n\n1. Se forem necessários dados de precisão conhecida para certas subdivisões do população, é aconselhável tratar cada subdivisão como uma “população” por direito próprio.\n\n2. Os problemas de amostragem podem diferir acentuadamente em diferentes partes da população. No caso das populações humanas, as pessoas que vivem em instituições (por exemplo, hotéis, hospitais, prisões) são frequentemente colocadas num estrato diferente das pessoas que vivem em casas normais porque uma abordagem diferente à amostragem é apropriada para as duas situações. Na amostragem de empresas, podemos possuir uma lista das grandes empresas, que são colocadas num estrato separado. Algum tipo de amostragem de área pode ter que ser usado para as empresas menores.\n\n3. A estratificação pode produzir um ganho de precisão nas estimativas das características de toda a população. Pode ser possível dividir uma população heterogênea em subpopulações, cada uma das quais internamente homogênea. Isto é sugerido pelo nome estratos, com a implicação de uma divisão em camadas. Se cada estrato for homogêneo, no sentido de que as medidas variam pouco de uma unidade para outra, uma estimativa precisa da média de qualquer estrato pode ser obtida a partir de uma pequena amostra desse estrato. Estas estimativas podem então ser combinadas numa estimativa precisa para toda a população.\n\n# 7\n\nSe você treina um modelo de Machine Learning (ou estadístico), como você identificaria se seu modelo tem uma alta variância (overfitting) ou um alto viés (bias, ou underfitting). Caso seu modelo apresenta alta variância, como você resolveria esse problema?\n\n::: callout-tip\n# Resposta\n:::\n\nEssa é uma questão de viés e variância em modelagem. Essa relação é fácil de ser observada na imagem abaixo.\n\n![Andrew Ng](img/vies.jpeg){fig-align=\"center\" width=\"300\"}\n\n\nA abordagem Bayesiana evita o overfitting, por mais que esse não seja necessariamente seu objetivo primordial, mas estamos condicionados a utilizar das formas ditas **regularizadoras**. Elas também chamadas de **penalizadoras** e são conhecidas por regressões LASSO e Ridge. É claro que a análise dos valores de VP, FN, VN e FP usando-se uma amostra de teste ou validação é crucial para evitar o overfitting, mas ainda assim, é importante estar atento a amostragem dos dados para compor a amostra de teste ou de validação.\n\nAcredito que deve haver vários outros métodos mais atualizados que esses mencionados acima, mas os básico as vezes já resolve o problema e também identifica a causa raiz, facilitando assim a procura de outros métodos ao invés de só ir aplicando vários métodos e escolher o que mais proporciona ganhos.\n\n\n\n\n\n# 8\n\nConsidere o seguinte modelo de regressão:\n\n- Salário = 1200 + 500.Idade + 600.Têm Faculdade + 50.Têm Linkedin\n\n::: panel-tabset\n\n# A\nInterprete o efeito da Idade na variável salário. \n\n::: callout-tip\n# Resposta\n:::\n\nNeste modelo, temos que a variável idade foi significativa (p-value = 0.023) com sinal positivo, ou seja, quanto maior a idade (aqui é importante ressaltar que se ela estiver categorizada, a interpretação é alterada), maior será a chance do Salário estar acima da média. Para este resultado é pertinente dizer que as empresas associam a idade a um bom salário, ou, assumem que quando o individuo é mais velho, este consequentemente deve ganhar mais em relação aos mais novos.\n\n# B\n\nQue acontece com as pessoas que não tem faculdade? \n\n::: callout-tip\n# Resposta\n:::\n\nA variável \"Têm faculdade\" foi significativa (p-value = 0.032) com sinal positivo, o que implica que esta variável auxilia no aumento do salário, em que quanto maior o seu nível de instrução, maior será a chance (600 vezes) do salário estar acima da média daqueles que não têm faculdade e isso pode ser recorrente ao fato desse indivíduo ter mais conhecimento sobre a área. Logo, de acordo com o modelo, aqueles que não têm faculdade estão propícios a receberem um salário abaixo da média em relação a aqueles que possuem faculdade.\n\n# C\n\nConsidere agora que o desvio padrão do coeficiente da variável \"Têm Linkedin\" é de 47, por tanto o seu p-valor é 0.92 (como visto na equação abaixo). O que isso implica para o modelo em questão? Essa variável é relevante?\n\n::: callout-tip\n# Resposta\n:::\n\nEssa variável não é relevante para predizer salários, mas veja que, apesar de não ser significativa, ela é positiva. Podemos iniciar com a problemática de que ter uma conta no linkedin não implica que seu salário será impactado por isso, mas podemos supor que o linkedin te ajuda positivamente em alguns aspectos, menos no salário. Aqui temos uma chance de analisar um pouco mais essa variável.\n\n:::\n\n\n:::\n",
    "supporting": [
      "Relatório_GAZEUS_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}