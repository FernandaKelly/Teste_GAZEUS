---
title: "Teste Técnico" # Título do relatório
subtitle: "**Posição de Data Analyst Jr - Buraco Jogatina**"
author: "Fernanda Kelly R. Silva" # Autor(a)
lang: pt 
date: "`r format(Sys.Date())`" 
date-format: short 
toc: true 
format: 
    html: 
      #css: ["custom.css"] 
      code-fold: false 
      code-tools: true  
      theme: 
        light: cosmo
        dark: superhero 
#title-block-banner: "#874a9c" 
code-annotations: hover 
execute:
  warning: false
  message: false
---

```{r}
#| echo: false

library(tidyverse)
```

# Tarefa 01: SQL

Um sistema de pagamentos, registra as transações dos seus usuários em duas tabelas:

Users: Contém o registro de usuários do sistema. Os seus atributos são:

-   id: Identificador único de usuário
-   age (idade): em anos
-   país de residência: país onde o usuários indicou que reside

Transactions: Que contém o registro de todas as transações que os usuários realizam através do sistema.

-   transaction_id: Identificador único da transação
-   transaction_date: Data na qual a transação foi realizada. O tipo é timestamp, ou seja, é um tipo de data que contém o ano, mês, dia, hora, minuto e segundos.
-   user_id: Identificador do usuário que realizou a transação (é uma chave externa à tabela users)
-   transaction_state: Campo que indica o estado da transação que pode ser *INITIATED*, *SUCCESS*, *FRAUD* or *CANCELLED*.
-   transaction_amount: Valor da transação em USD

Com estes dados disponíveis, precisamos gerar queries SQL para responder às seguintes perguntas. **Para fins didáticos, criei esses bancos de dados para simular a resolução das questões.**

### tabela_transactions

```{r}
#| echo: false
tabela_transactions <- tibble::tribble(
                         ~user_id, ~transaction_date, ~transaction_id, ~transaction_state, ~transaction_amount,
                         "1", "2024-03-12 15:08:49 UTC", "255", "sucesso", "119.65",
                         "2", "2024-03-23 20:08:03 UTC", "585", "INITIATED", "22.63",
                         "3", "2024-03-18 10:56:25 UTC", "178", "CANCELLED", "225.87",
                         "4", "2024-03-12 08:21:32 UTC", "887", "SUCCESS", "16.58",
                         "5", "2024-03-30 05:24:49 UTC", "569", "FRAUD", "1225.89",
                         "6", "2024-03-21 12:32:51 UTC", "789", "SUCCESS", "59.90",
                         "7", "2024-03-10 08:25:49 UTC", "236", "FRAUD", "256.98",
                         "7", "2024-03-10 05:21:49 UTC", "458", "FRAUD", "986.69",
                         "7", "2024-03-09 01:08:49 UTC", "125", "FRAUD", "1256.89",
                         ) %>% 
                      dplyr::mutate(transaction_amount = as.numeric(transaction_amount),
                                    data_curta = as.Date(transaction_date)) 
```

```{r}
knitr::kable(tabela_transactions)
```

### tabela_user

```{r}
#| echo: false
tabela_user <- tibble::tribble(
                         ~id, ~age, ~pais, 
                         "1", "18", "Brasil",
                         "2", "29", "Estados Unidos",
                         "3", "55", "África do Sul",
                         "4", "32", "Bolívia",
                         "5", "60", "Brasil",
                         "6", "42", "Brasil",
                         "7", "38", "África do Sul",
                         ) 
```

```{r}
knitr::kable(tabela_user)
```

Para que a experiência do SQL seja real, vou conectar as bases com o SQLite.

```{r}
library(dbplyr)
library(dplyr)

con <- DBI::dbConnect(RSQLite::SQLite(), ":memory:")
copy_to(con, tabela_user)
copy_to(con, tabela_transactions)

tabela_user <- tbl(con, "tabela_user")                 # <1>
tabela_transactions <- tbl(con, "tabela_transactions") # <2>
```

1.  Conectando a base de dados {tabela_user} com o SQLite
2.  Conectando a base de dados {tabela_transactions} com o SQLite

::: panel-tabset
# A

Qual é a idade média de usuários do sistema por país?

```{r}
#| echo: false
tabela_A <- tabela_user %>%
                     dplyr::group_by(pais) %>% 
                     dplyr::summarise(media_idade = round(mean(age, na.rm = TRUE),2))

# tabela_A %>%
#   show_query()
```

::: callout-tip
# Resposta
:::

``` sql
<SQL>
SELECT `pais`, ROUND(AVG(`age`), 2) AS `media_idade`
FROM `tabela_user`
GROUP BY `pais`
```

# B

Qual é o país com a maior quantidade de dinheiro transacionado (considere só transações finalizadas com sucesso ou "SUCCESS")?

Como vamos precisar do merge das duas tabelas, vou criar a **tabela_merge** sem algum filtro para facilitar as consultas e não alocar memória desnecessária.

```{r}
tabela_merge <- tabela_transactions %>%
                 dplyr::left_join(tabela_user, by = c("user_id" = "id")) 
```

```{r}
#| echo: false
tabela_B <- tabela_merge %>% 
                  dplyr::filter(transaction_state == "SUCCESS" | transaction_state == "sucesso") %>% 
                  dplyr::slice_max(transaction_amount)

# tabela_B %>%
#   show_query()
```

::: callout-tip
# Resposta
:::

``` sql
<SQL>
SELECT
  `user_id`,
  `transaction_date`,
  `transaction_id`,
  `transaction_state`,
  `transaction_amount`,
  `data_curta`,
  `age`,
  `pais`
FROM (
  SELECT `q01`.*, RANK() OVER (ORDER BY `transaction_amount` DESC) AS `col01`
  FROM (
    SELECT `tabela_transactions`.*, `age`, `pais`
    FROM `tabela_transactions`
    LEFT JOIN `tabela_user`
      ON (`tabela_transactions`.`user_id` = `tabela_user`.`id`)
  ) AS `q01`
  WHERE (`transaction_state` = 'SUCCESS' OR `transaction_state` = 'sucesso')
) AS `q01`
WHERE (`col01` <= 1)
```

# C

Qual é o país com maior taxa de fraude em porcentagem respeito ao número de transações totais no país?

```{r}
#| echo: false
library(janitor)
tabela_C <- tabela_merge %>% 
              dplyr::group_by(transaction_state) %>% 
              dplyr::mutate(taxa = transaction_amount/sum(transaction_amount)) %>% 
              dplyr::ungroup() %>%
              dplyr::filter(transaction_state == "FRAUD") %>% 
              dplyr::slice_max(taxa)
# tabela_C %>%
#   show_query()
```

::: callout-tip
# Resposta
:::

``` sql
<SQL>
SELECT
  `user_id`,
  `transaction_date`,
  `transaction_id`,
  `transaction_state`,
  `transaction_amount`,
  `data_curta`,
  `age`,
  `pais`,
  `taxa`
FROM (
  SELECT `q01`.*, RANK() OVER (ORDER BY `taxa` DESC) AS `col01`
  FROM (
    SELECT
      `q01`.*,
      `transaction_amount` / SUM(`transaction_amount`) OVER (PARTITION BY `transaction_state`) AS `taxa`
    FROM (
      SELECT `tabela_transactions`.*, `age`, `pais`
      FROM `tabela_transactions`
      LEFT JOIN `tabela_user`
        ON (`tabela_transactions`.`user_id` = `tabela_user`.`id`)
    ) AS `q01`
  ) AS `q01`
  WHERE (`transaction_state` = 'FRAUD')
) AS `q01`
WHERE (`col01` <= 1)
```

# D

Na mesma linha da pergunta anterior, responda qual é a faixa de idade de usuários que mais cometem fraude (em percentagem). Separe as faixas etárias em: (\<18 anos, 18-30 anos, 30 - 45 anos, 45 - 60 anos, 60 \> anos) e considere que para responder essa pergunta, você deverá considerar o fato que um usuário pode ter executado várias transações, das quais poucas (ou muitas) podem ter sido fraude entre as demais.

```{r}
#| echo: false
tabela_D <- tabela_merge %>% 
             dplyr::mutate(age_cat = dplyr::case_when(age < 18 ~ "menor que 18 anos",
                                                      age >= 18 & age < 30 ~ "18-29 anos",
                                                      age >= 30 & age < 45 ~ "30-44 anos",
                                                      age >= 45 & age < 60 ~ "45-59 anos",
                                                      age >= 60  ~ "maior igual a 60 anos"),
                           taxa_idade = dplyr::if_else(transaction_state == "FRAUD", transaction_amount/sum(transaction_amount), 0))  
          
# tabela_C %>%
#   show_query()
```

::: callout-tip
# Resposta
:::

``` sql
```

# E

Imagine que a camada executiva da empresa dona do sistema, precisa criar um Dashboard para monitorar o estado das transações nos últimos 3 dias. Para isso você precisa criar uma query SQL que calcule o número e dinheiro das transações não finalizadas, número e dinheiro de transações finalizadas com sucesso (SUCCESS), o número e dinheiro de transações canceladas (CANCELLED), o número e dinheiro de fraudes (FRAUD), agrupado por país, nos 3 dias anteriores de quando o executivo da empresa consulte seu Dashboard.

```{r}
#| echo: false
tabela_E <- tabela_merge %>% 
              dplyr::mutate(tres_n = dplyr::if_else((lubridate::today() - 1) == data_curta, 1,
                                                    dplyr::if_else((lubridate::today() - 2) == data_curta, 1,
                                                                   dplyr::if_else((lubridate::today() - 2) == data_curta,1,0)))) %>% 
              dplyr::filter(tres_n == 1) %>% 
              dplyr::mutate(transaction_state = dplyr::if_else(transaction_state == "sucesso", "SUCCESS", transaction_state)) %>% 
              dplyr::group_by(pais, transaction_state) %>% 
              dplyr::mutate(n_transacoes =  n(),
                            dinheiro = sum(transaction_amount))

# tabela_E %>%
#   show_query()
```

::: callout-tip
# Resposta
:::

``` sql
<SQL>
SELECT
  `q01`.*,
  COUNT(*) OVER `win1` AS `n_transacoes`,
  SUM(`transaction_amount`) OVER `win1` AS `dinheiro`
FROM (
  SELECT
    `user_id`,
    `transaction_date`,
    `transaction_id`,
    CASE WHEN (`transaction_state` = 'sucesso') THEN 'SUCCESS' WHEN NOT (`transaction_state` = 'sucesso') THEN `transaction_state` END AS `transaction_state`,
    `transaction_amount`,
    `data_curta`,
    `age`,
    `pais`,
    `tres_n`
  FROM (
    SELECT
      `q01`.*,
      CASE WHEN ((DATE('now') - 1.0) = `data_curta`) THEN 1.0 WHEN NOT ((DATE('now') - 1.0) = `data_curta`) THEN (CASE WHEN ((DATE('now') - 2.0) = `data_curta`) THEN 1.0 WHEN NOT ((DATE('now') - 2.0) = `data_curta`) THEN (CASE WHEN ((DATE('now') - 2.0) = `data_curta`) THEN 1.0 WHEN NOT ((DATE('now') - 2.0) = `data_curta`) THEN 0.0 END) END) END AS `tres_n`
    FROM (
      SELECT `tabela_transactions`.*, `age`, `pais`
      FROM `tabela_transactions`
      LEFT JOIN `tabela_user`
        ON (`tabela_transactions`.`user_id` = `tabela_user`.`id`)
    ) AS `q01`
  ) AS `q01`
  WHERE (`tres_n` = 1.0)
) AS `q01`
WINDOW `win1` AS (PARTITION BY `pais`, `transaction_state`)
```
:::

# Tarefa 02: Python

```{r}
#| echo: false
library(reticulate)
```

Numa tribo ancestral e muito desenvolvida, orientada principalmente por uma cultura lógica e matemática, os nativos estão interessados no estudo das linguagens de outras tribos e civilizações.

Durante seus estudos, estes perceberam que as linguagens dessas outras civilizações são compostas por um conjunto de símbolos (letras do alfabeto) que são agrupados e combinados para representar conceitos (palavras). Assim mesmo, estes também sabem que cada civilização possui um alfabeto específico.

Na tentativa da tribo de avançar nos seus estudos, estes desejam saber qual seria o número de palavras possíveis a serem criadas em função do tamanho da palavra e o conjunto de símbolos (alfabeto) da civilização em estudo, independentemente se essas "palavras" representam algum significado ou não.

::: panel-tabset
# A

Você, que compartilha os atributos da tribo enquanto as capacidades analíticas e lógicas, precisa ajudá-la escrevendo um algoritmo para fazer o cálculo descrito acima, considerando que tribo oferecerá para você o alfabeto da civilização e o tamanho da palavra/combinação. Seu algoritmo deveria ser capaz de calcular que o número total de palavras possíveis (independente se tem significado ou não) é de 16.

OBS: NÃO PRECISA ESCREVER A LISTA DAS PALAVRAS, A TRIBO SÓ PRECISA DO NÚMERO!

::: callout-tip
# Resposta

Esse é um claro problema de análise combinatória com reposição/reposição.
:::

```{python}
from itertools import combinations_with_replacement,chain

# Obtem todas as combinações de [1, 2, 3] e tamanho 2
comb_A = combinations_with_replacement([1, 2, 3], 2)
print(len(list(comb_A)))
```

# B

Para ajudar a tribo ainda mais! Você deverá modificar seu algoritmo (ou talvez escrever um novo) para fazer o mesmo cálculo, só que agora, as palavras não podem ter símbolos repetidos.

::: callout-tip
# Resposta

Esse também é um claro problema de análise combinatória, mas, nesse caso, é um problema em que **não há reposição**.
:::

```{python}
from itertools import combinations

# Obtem todas as permutações de {@,d,2,b}  
comb_B = combinations([1, 2, 3], 2)

print(len(list(comb_B)))
```
:::

# Tarefa 03: Estatística e Machine Learning

::: panel-tabset
# 1

Dado um dataset das estaturas (em centímetros) de 13 indivíduos:

Dataset = {175, 166, 183, 193, 155, 177, 173, 171, 162, 185, 176, 161, 188}

Calcule:

-   a média
-   a mediana
-   20 percentil
-   80 percentil
-   desvio padrão

Considerando os dados, responda a que distribuição (paramétrica) os dados se aproximam.

::: callout-tip
# Resposta
:::

```{r}
#| echo: false
dataset <- tibble::tribble(
                         ~altura, 
                         175,
                         166,
                         183,
                         193,
                         155,
                         177,
                         173,
                         171,
                         162,
                         185, 
                         176, 
                         161, 
                         188,
                         )
```

```{r}
#| echo: false
library(ggplot2)
library(dplyr)
library(tidyverse)
library(knitr)

summarise_dataset <- dataset %>% 
  dplyr::summarise(vinte_percentil = quantile(altura, probs = 0.20, na.rm = TRUE),
                   mean = mean(altura, na.rm = TRUE),
                   median = median(altura, na.rm = TRUE),
                   oitenta_percentil = quantile(altura, probs = 0.80, na.rm = TRUE),
                   desvio_padrao = sd(altura, na.rm = TRUE)
                   ) 

knitr::kable(summarise_dataset)
```

De acordo com o gráfico abaixo, a distribuição da variável **altura** pode ser descrita por uma normal, mas para fins de decisão, deve-se munir de testes paramétricos ou não paramétricos. Veja que sua variabilidade indica que os valores observados tendem a estar distantes da média – ou seja, a distribuição é mais “espalhada”.

```{r}
#| echo: false
library(ggplot2)

  ggplot2::ggplot(dataset) +
  ggplot2::aes(x = altura) +
  ggplot2::geom_density(adjust = 1L, fill = "#EF562D") +
  ggplot2::labs(x = "Altura (cm)", 
                title = "Densidade: Altura (cm)") +
  ggplot2::geom_vline(xintercept = 174.23, linetype = "dashed", show.legend = TRUE) +
    ggplot2::geom_vline(xintercept = 174.23, linetype = "dashed", show.legend = TRUE) +
    ggplot2::geom_vline(xintercept = 175, linetype = "dashed", show.legend = TRUE) +
    ggplot2::geom_vline(xintercept = 163.6, linetype = "dashed", show.legend = TRUE) +
    ggplot2::geom_vline(xintercept = 184.2, linetype = "dashed", show.legend = TRUE) +
  
  ggplot2::theme_gray()
```

```{r}
qqnorm(dataset$altura)
qqline(dataset$altura, col=2)
```

# 2

Com suas próprias palavras, explique em que consiste o teorema central do limite, e se possível, mencione a sua importância no campo da inferência estatística.

::: callout-tip
# Resposta
:::

O Teorema do limite central (TLC) basicamente demonstra a **tendência** de aproximação das variáveis aleatórias com a distribuição normal, em que a aproximação da distribuição Normal melhora na medida que se fizesse a média do lançamento de mais dados. Esse comportamento pode ser visualizado na imagem abaixo. Na estatística, há provas que um n igual a 30 é suficientemente grande para que essa amostra seja descrita com uma distribuição normal e, por isso, é fácil encontrar análises clássicas com todos os pressupostos violados por seus pesquisadores devido a esta pressuposição do n igual a 30.

![Teorema do Limite Central](img/img213.png){fig-align="center" width="300"}

# 3

Suponha que numa escola, 2 grupos diferentes de estudantes (Grupo A e Grupo B) fazem o mesmo teste de matemáticas. As pontuações para cada grupo são dados pelos seguintes datasets:

-   Grupo A: {80, 85, 88, 90, 92, 75, 78}
-   Grupo B: {75, 78, 82, 85, 87, 93, 99}

Elabore um teste de hipótese para determinar se existe uma diferença estatisticamente significativa entre a média das pontuações dos dois grupos com uma confiança de 95%.

::: callout-tip
# Resposta
:::

```{r}
#| echo: false
tabela_estudantes <- tibble::tribble(
                         ~nota, ~grupo, 
                         80, "A", 
                         85, "A",
                         88, "A",
                         90, "A",
                         92, "A",
                         75, "A",
                         78, "A",
                         75, "B",
                         78, "B",
                         82, "B",
                         85, "B",
                         87, "B",
                         93, "B",
                         99, "B",
                         ) %>% dplyr::mutate(grupo = factor(grupo, levels = c("A", "B")))
```

**Antes de qualquer levantamento de hipótese, é bom analisar as medidas de tendência central de cada grupo e também graficamente** e, de acordo com a tabela abaixo, não há muitas evidências de que essa média entre os grupos seja diferente, mas é evidente que o **grupo B** tem uma variabilidade maior em relação as notas (desvio padrão = 8.363754).

```{r}
#| echo: false

summarise_estudantes <- tabela_estudantes %>% 
  dplyr::group_by(grupo) %>% 
  dplyr::summarise(vinte_percentil = quantile(nota, probs = 0.20, na.rm = TRUE),
                   mean = round(mean(nota, na.rm = TRUE),2),
                   median = median(nota, na.rm = TRUE),
                   oitenta_percentil = quantile(nota, probs = 0.80, na.rm = TRUE),
                   desvio_padrao = round(sd(nota, na.rm = TRUE),2)
                   ) 

knitr::kable(summarise_dataset)
```
```{r}
#| echo: false
library(ggplot2)

ggplot(tabela_estudantes) +
 aes(x = nota, y = grupo, fill = grupo) +
 geom_boxplot() +
 scale_fill_manual(values = c(A = "#FFF5EB", 
B = "#E56B0D")) +
 labs(x = "Nota", y = "Grupo") +
 theme_gray() +
 theme(legend.position = "top", axis.title.y = element_text(face = "bold", 
 hjust = 0))
```

É importante ressaltar que a verificação dos pressupostos é de extrema relevância para que os insights e conclusões sejam verdadeiramente corretos. Neste casso, estamos **supondo ** que as duas amostras são independentes e, em caso de teste de médias, usaríamos o teste t independente que apresenta dois **pressupostos**:

- **Normalidade da variável dependente de cada grupo**
- **Homogeneidade de variâncias (ou seja, grupos com variâncias homogêneas)**

### Normalidade

Para verificar a normalidade dos dados, é recomendável utilizar o teste de **Shapiro WIlk** quando o número de amostra é menor que 50. A estatı́stica T (ou T ) é basicamente o quadrado de um coeficiente de correlação, onde o coeficiente de correlação de Pearson é calculado entre a estatı́stica ordenada X i na amostra e o escore ai , que representa o que a estatı́stica ordenada deveria parecer se a população é Normal. Portanto, as hipóteses levantadas são:

$H_{0}$ : F(x) é função de distribuição normal;
$H_{1}$ : F(x) não é função de distribuição normal;

```{r}
#| echo: false
 
teste_shapiro <- tabela_estudantes %>% 
  dplyr::group_by(grupo) %>% 
  rstatix::shapiro_test(nota)

knitr::kable(teste_shapiro)
```


Veja que de acordo com o teste de Shapiro Wilk, ao nível de 95% de significância, **não rejeita-se a hipótese nula**, ou seja, a variável **notas** é descrita por uma distribuição normal.


### Variância

O segundo pressuposto pode ser avaliado pelo **teste de Levene**, em que as hipóteses são:

$H_{0}$ : Os grupos apresentam variâncias homogêneas;
$H_{1}$ : Os grupos não apresentam variâncias homogêneas;

```{r}
#| echo: false

teste_levene <- car::leveneTest(nota ~ grupo, data = tabela_estudantes, center = mean)

knitr::kable(teste_levene)
```

Logo, ao nível de 95% de significância, **não rejeitamos a hipótese nula**, ou seja, os grupos apresentam variâncias homogêneas e assim podemos seguir com o teste t independente.

Seguindo os passos para elaboração de um teste de hipótese:

1.  Enunciar as Hipóteses:

$H_{0}$ : Não há diferença média entre os grupos; 
$H_{1}$ : Há diferença média entre os grupos;

2. Fixar o nı́vel de significância $\alpha$) do teste:

**No nosso caso, o nível de significância será 95%.**

- $\alpha$ : Erro tipo I: Rejeitar $H_{0}$  quando é verdadeira.


3. Escolha da Estatı́stica teste adequada:

Para responder a essa pergunta, usaremos o teste-t independente. Note que queremos comparar as notas (variáveis numéricas) de dois grupos independentes de alunos (A x B).

4. Obtenção dos valores crı́ticos e Obtenção da Estatı́stica calculada:

```{r}
#| echo: false
teste_t <- stats::t.test(nota ~ grupo, tabela_estudantes, var.equal=TRUE)
teste_t
```

6. Conclusão e tomada de decisão

O teste resulta em um valor de t (no caso, t = -0.3935) e um valor de p (p-value) que é calculado com base nesse valor de t e nos graus de liberdade (df, do inglês degrees of freedom). Nesse caso, o valor de p é: 0.7008, um valor superior ao nível de significância 0.05. Como p > 0.05, não vamos rejeitar a $H_{0}$ e considerar que os dois grupos que apresentam notas, em média, não são estatisticamente diferentes. Veja que o IC 95% para a diferença entre médias inclui o zero: [-10.27; 7.12].



# 4

Determine a hipótese nula (Ho) e hipótese alternativa (Ha) do seguinte cenário: Uma empresa afirma que o tempo médio dos seus produtos é menos de 4 dias. Você, conta com uma amostra dessas entregas para validar estadísticamente essa afirmação.

::: callout-tip
# Resposta
:::

# 5

Calcule o coeficiente de correlação de pearson das variáveis "Número de aparições Zendaya em filmes por ano" e "Número de pessoas afogadas em piscinas no Brasil ao ano". Em base nos seus resultados, considera que a variável "Número de aparições Zendaya em filmes por ano" é um bom preditor do número de pessoas afogadas no Brasil? Justifique a sua resposta.

::: callout-tip
# Resposta
:::

# 6

Explique a diferença entre amostragem estratificada e amostragem randômica ou aleatória. Discuta quais são as vantagens e desvantagens de cada uma e dê exemplos de casos onde uma abordagem é mais adequada que a outra.

::: callout-tip
# Resposta
:::

# 7

Se você treina um modelo de Machine Learning (ou estadístico), como você identificaria se seu modelo tem uma alta variância (overfitting) ou um alto viés (bias, ou underfitting). Caso seu modelo apresenta alta variância, como você resolveria esse problema?

::: callout-tip
# Resposta
:::

# 8

Considere o seguinte modelo de regressão:

-   Salário = 1200+ 500.Idade + 600. Têm Faculdade + 50.Têm Linkedin

Interprete o efeito da Idade na variável salário. Que acontece com as pessoas que não tem faculdade? Considere agora que o desvio padrão do coeficiente da variável "Têm Linkedin" é de 47, por tanto o seu p-valor é \~0.92 (como visto na equação abaixo). O que isso implica para o modelo em questão? Essa variável é relevante?

::: callout-tip
# Resposta
:::
:::
